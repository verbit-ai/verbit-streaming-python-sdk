# Verbit Streaming Python SDK

***TODO : 'CI Badge'***
[![CIRCLE-CI::verbit-ai](https://circleci.com/gh/verbit-ai/verbit-streaming-python-sdk/tree/master.svg?style=shield)](https://app.circleci.com/pipelines/github/verbit-ai/verbit-streaming-python-sdk)

[comment: Or Github Actions setup..]


## Purpose

This package is the reference implementation for Verbit's Streaming API.

It is a client SDK for streaming media to, and getting responses from Verbit's
Speech Recognition services, via a standard WebSocket connection.

You can use it as-is (see installation instructions below), or use it as
a Python implementation example, and implement the client yourself.


## Documentation

See our [API docs](https://www.XXXX.ai/docs) for more information about the API and
more python examples.

## Installation

To install the package via PyPi simply run:  ***TODO : Decide on package name***

    # pip install --upgrade verbit-speech-python
    pip install --upgrade verbit-streaming-sdk

[comment]: <> (Install from source with:)

[comment]: <> (    python setup.py install)

### Requirements

- Python 3.8+  ***TODO : We can easily reduce this requirement to Python 3.6***

## Usage

To start using the client, you will need an Access Token, which is currently
generated by our [Booking System](https://www.link-to-booking.co).

Once Create a client with the
generated Access Token:

```python
from verbit.streaming_client import SpeechStreamClient

client = SpeechStreamClient(access_token="ACCESS TOKEN")
```

### Streamed audio and responses:

Create a generator function yielding audio chunks of type `byte` in order to provide an audio-stream to the client-SDK.

Current SDK version only supports 16-bit signed-little-endian PCM input from this generator.

The following example streams audio from a PCM-`wave` file:

```python
from time import sleep
from verbit.streaming_client import SpeechStreamClient

CHUNK_DURATION_SECONDS = 0.1
media_path = 'example.wav'

def media_generator_wavefile(wav_path, chunk_duration):
    """
    Example generator, for streaming a 'WAV' audio-file, simulating realtime playback-rate using sleep()
    """

    # calculate chunk size
    # Note: assuming input file is a 16-bit mono 16000Hz WAV file
    chunk_size = int(chunk_duration * 16000)

    with open(str(wav_path), 'rb') as wav:
        while chunk_bytes := wav.read(chunk_size):
            yield chunk_bytes
            sleep(chunk_duration)

media_generator = media_generator_wavefile(media_path, CHUNK_DURATION_SECONDS)

response_generator = client.start_stream(media_generator=media_generator)
```

The resulting `response_generator` is another generator-function provided by the SDK, from which you can consume responses, there are generally two types of responses: captions and updating-transcriptions:
[WIP]
```python
# get transcription responses
print('Listening for responses ...')
for response in response_generator:
    alternatives = response['response']['alternatives']
    alt0_transcript = alternatives[0]['transcript']
    print(alt0_transcript)
```

-----
[comment: future extension: remove for now...]

cut cut cut ---
------

### Getting your transcript

[comment: would we like to provide this??]
Once your file is transcribed, you can get your transcript in a few different forms:

```python
# as text
transcript_text = client.get_transcript_text(job.id)

# as json
transcript_json = client.get_transcript_json(job.id)

# or as a python object
transcript_object = client.get_transcript_object(job.id)
```

### Getting captions output

[comment: would we like to provide this??]

```python
captions = client.get_captions(job.id, content_type=CaptionType.SRT)
```

### Streamed outputs



----
## (REV) Streaming audio
[comment: REV's example: a nice way to document the full 'on_*' functions]

```python
from rev_ai.streamingclient import RevAiStreamingClient
from rev_ai.models import MediaConfig

#on_error(error)
#on_close(code, reason)
#on_connected(id)

config = MediaConfig()
streaming_client = RevAiStreamingClient("ACCESS TOKEN",
                                        config,
                                        on_error=ERRORFUNC,
                                        on_close=CLOSEFUNC,
                                        on_connected=CONNECTEDFUNC)
```

`on_error`, `on_close`, and `on_connected` are optional parameters that are functions to be called when the websocket errors, closes, and connects respectively. The default `on_error` raises the error, `on_close` prints out the code and reason for closing, and `on_connected` prints out the job ID.
If passing in custom functions, make sure you provide the right parameters. See the sample code for the parameters.

Once you have a streaming client setup with a `MediaConfig` and access token, you can obtain a transcription generator of your audio. You can also use a custom vocabulary with your streaming job by supplying the optional `custom_vocabulary_id` when starting a connection!

`response_generator` is a generator object that yields the transcription results of the audio including partial and final transcriptions. The `start` method creates a thread sending audio pieces from the `AUDIO_GENERATOR` to our
[streaming] endpoint.

If you want to end the connection early, you can!

```python
streaming_client.end()
```
